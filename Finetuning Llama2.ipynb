{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1> Finetuning Llama2 7b on the TweetSumm customer support dialog summary dataset</h1>","metadata":{}},{"cell_type":"markdown","source":"**1. We start by installing the necessary packages and libraries and their revelant versions to avoid any issues related to version updates.**","metadata":{}},{"cell_type":"code","source":"!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7\n!pip install wandb\n!pip install huggingface-hub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2. We login to huggingface_hub using a personal account to import the formatted instruction dataset and the Llama2 base model.**","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(new_session=False,\nwrite_permission=True,\ntoken='...',\nadd_to_git_credential=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**3. We create 3 variables containing the respective names of the base model, the result model, and the instruction dataset.**","metadata":{}},{"cell_type":"code","source":"model_name = \"meta-llama/Llama-2-7b-chat-hf\"\nnew_model = \"Llama2-7b-Dialog-Summary\"\ndataset_name = \"Marouane50/Dialog-Summarization-Dataset-Formatted\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**4. We load the instruction dataset and store the training and validation sets in two variables.\n**","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\ntraining_dataset = load_dataset(dataset_name, split=\"training\")\nvalidation_dataset = load_dataset(dataset_name, split=\"validation\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**5. To reduce the memory and computational costs, we decided to integrate quantization into the model by representing the modelâ€™s weights with lower-precision data types. So we use 4-bit quantization (nf4), and double quantization to compress the base model like stated in the QLora paper.**","metadata":{}},{"cell_type":"code","source":"import torch\nimport os\nfrom transformers import BitsAndBytesConfig\n\ncompute_dtype = getattr(torch, \"float16\") # Imports the 16-bit floating point data from the PyTorch library\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit = True, # Loads the model with 4-bit quantization to reduce its memory footprint\n    bnb_4bit_quant_type=\"nf4\", # Type of 4-bit quantization to use\n    bnb_4bit_compute_dtype= compute_dtype, # Sets the float16 as data type to use for computations\n    bnb_4bit_use_double_quant=True # Enables double quantization \n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**6. We load the base model that we are going to use for the training, by importing from huggingface. Then we configure some of its parameters to recommended values in the context of finetuning.\n**","metadata":{}},{"cell_type":"code","source":"# Loading the base model with QLoRA config\nfrom transformers import AutoModelForCausalLM\n\n#We use the AutoModelForCausalLM clas from the transformers library as an architecture to load the model as it's adapted for text generation tasks.\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config = bnb_config,\n    device_map = {\"\": 0},\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**7. We load the tokenizer of the model we're using, and we configure it to avoid token overflow**","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# Load LLaMA tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, add_eos_token=True, use_fast=True)\n\n#This code was added as after a recommendation to stop the model from generating text and avoid overflow cases.\ntokenizer.pad_token_id = 18610\ntokenizer.padding_side = \"right\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**8. We setup the Lora parameters as stated in the LoRA paper. Lora accelerates the finetuning of large language models while consuming less memory. We create a parameter-efficient fine-tuning configuration where we define LoRA specific parameters. LoRA is a method that accelerates the finetuning of large language models by: a)Tracking changes to weights instead of updating them directly. b)Decomposing large matrices of weight changes into smaller matrices that contains the trainable parameters.**","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig\n\npeft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**9. We create a TrainingArguments class which contains all the parameters we can adjust as well as flags for activating different training options. There are built-in default training parameters, and we can optimisee the training by setting new parameters. The reference values for each parameter is available in the Trainer class documentation (https://huggingface.co/transformers/v3.0.2/main_classes/trainer.html#tftrainingarguments), the values were slightly adjusted to improve the training loss and validation loss for the fine-tuning.**\n","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments\n\n# Settin the training parameters\ntraining_arguments = TrainingArguments(\n    output_dir=\"./results\", # Folder where the training files will be saved\n    num_train_epochs=2, # Number of interation through the training dataset\n    per_device_train_batch_size=4, # Number of data samples processed before the model is updated\n    gradient_accumulation_steps=1,\n    optim=\"paged_adamw_32bit\", # Type of optmizer used\n    save_steps=0,\n    evaluation_strategy=\"steps\", \n    eval_steps = 0.1, # Frequency at which the evaluation loss is computed\n    logging_steps=20, # Sets how often logging information is recorded\n    learning_rate=2e-4,# Allows the optimizer to adjust the learning speed for each iteration\n    weight_decay=0.001,\n    fp16=False,\n    bf16=False,\n    max_grad_norm=0.3,\n    max_steps=-1,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"cosine\", # Type of learning rate chosed by the scheduler for the training.\n    report_to=\"wandb\" # Visualisation library for training metrics report\n)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**10. We initialize an instance of SFTTrainer, to configure the parameters of the self-supervised learning for transformers, used fot fine-tuning.**","metadata":{}},{"cell_type":"code","source":"from trl import SFTTrainer\n# Setting the fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model, \n    train_dataset=training_dataset, # Training set\n    eval_dataset=validation_dataset, # Validation set\n    peft_config=peft_config, # LoRA configuration\n    dataset_text_field=\"text\", #\n    max_seq_length=4096, # Optional maximum tokens length of the sequences used during the training\n    tokenizer=tokenizer, # Tokenizer for the base model used\n    args=training_arguments, \n    packing=False,\n)\n\n# Fine-tuning the model with the previously set training parameters\ntrainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Saving the trained model\ntrainer.save_model(new_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**11. We run the text generation pipeline with our base model using the Llama2 chat prompt format.**","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\n# Default system prompt for a dialog summarization task\nsystem_prompt = \"The following text is a conversation between a human and an AI agent. Write a summary of the conversation.\"\n\n# The instruction text\nprompt = \"\"\"user: Hello, i'm looking to get a refund for a computer that i bought from your store. /n agent: Could you please let me know the reason why you want a refund ? /n user: The screen was broken. /n agent: Ok, i will send your request to the customer service department and you will be notified in 5 working days. /n user: Ok, thank you.\"\"\"\n\n# The pipeline function from the transformers library allows to start using the model\npipe = pipeline(\n    task=\"text-generation\", # Type of task the model will have to perform\n    model=model, # Model used in the pipeline\n    tokenizer=tokenizer, # Tokenizer compatible with the model\n    max_length=300 # Optional, limits the number of characters in the output\n)\n\nresult = pipe(f\"[INST] <<SYS>> {system_prompt} <</SYS>> {prompt} [/INST]\")  # We pass the formatted prompt to the pipeline function\nprint(result[0]['generated_text']) # The result is in the form of a list, we select the generated_text entry to get the model's output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>12. Reload model in FP16 and merge it with LoRA weights, this is when we incorporatethe trained weights in the base model, which results in the fine-tuned model. </b>","metadata":{}},{"cell_type":"code","source":"from peft import PeftModel\n\n# Lora Configuration\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    low_cpu_mem_usage=True, # Optimizes memory usage during loading \n    return_dict=True, # Dictionary output format\n    torch_dtype=torch.float16, # Loads the model with 16-bit floating point LoRAprecision to reduce memory usage and computation time\n    device_map={\"\": 0},\n)\n\nmodel = PeftModel.from_pretrained(base_model, new_model) # Loads a parameter-efficient finetuning model using the base model and trained model weights \nmodel = model.merge_and_unload() # This methods merges the weights from the trained model with the base mode\n\n# We reload the tokenizr of Llama2 model to be used in the new text generation pipeline\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.padding_side = \"right\" # Line of code to avoid overflow during text generation","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**13. After merging the weights, we run the text generation pipeline in with our new fine-tuned model.**","metadata":{}},{"cell_type":"code","source":"system_prompt = \"The following text is a conversation between a user and an AI agent. Write a summary of the conversation.\"\n#prompt = \"user: Hello, i'm looking to get a refund for a computer that i bought from your store. agent: Could you please let me know the reason why you want a refund ? user: The screen was broken. agent: Ok, i will send your request to the customer service department and you will be notified in 5 working days. user: Ok, thank you.\"\nprompt = \"user: SO I Can't castfrom my app to my TV?Really? agent: As long as both devices are connected to your home network you should be able to cast content. If either device is not connected to your home network then casting is blocked due to our network agreements. ^RT user: Spectrum Live TV and Spectrum Internet. But No option to cast. It's frustrating. agent: It should be under Settings, then Display, then Cast. You should see a list of compatible devices on your network to cast to. The instructions for Apple Airplay are different and can be found here as well. ^RT user: Nah. Not even an option. See agent: Are you streaming to a chromecast or directly to the TV? ^RT user: It's the APP. I'm wanting to stream/cast to the Chromecast. agent: Are you able to reboot your modem and then log out of the app and see if the display option comes up for you when you log back in? ^RT user: I will attempt that. Give me a few minutes to see if that helps. agent: Sure thing. I'll be here once everything is done rebooting. user: There is not a 'Display' setting under settings even after reboot of everything. agent: I would be happy to get this issue escalated for you. Can you please DM the service address and phone number?\"\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=1000)\nresult = pipe(f\"<s>[INST] <<SYS>> {system_prompt} <</SYS>> {prompt} [/INST]\")\nprint(result[0]['generated_text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**14. We store the trained model and tokenizer in Huggginface Hub to evaluate it with the ROUGE benchmark**","metadata":{}},{"cell_type":"code","source":"model.push_to_hub(\"Marouane50/Llama2-Dialog-Summarization\", check_pr=True)\n\ntokenizer.push_to_hub(\"Marouane50/Llama2-Dialog-Summarization\",check_pr=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"References:\nTraining Arguments: https://huggingface.co/transformers/v3.0.2/main_classes/trainer.html#tftrainingarguments)\nFintuning : https://huggingface.co/docs/transformers/training\nOpen-source code: https://colab.research.google.com/drive/1p68M5E5fZ7kSa7nA-e-20489nuFSXVp2?usp=sharing","metadata":{}}]}
