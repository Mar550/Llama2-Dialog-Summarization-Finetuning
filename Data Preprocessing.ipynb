{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<b> 1. LOADING THE DATASET </b>","metadata":{}},{"cell_type":"markdown","source":"We import the dataset from the referenced open-source repository.","metadata":{}},{"cell_type":"code","source":"# We login to hugging face hub to import the dataset\nfrom huggingface_hub import login\nlogin(new_session=False,\nwrite_permission=True,\ntoken='...',\nadd_to_git_credential=True)\n\nfrom datasets import load_dataset\ndataset = load_dataset(\"Salesforce/dialogstudio\", \"TweetSumm\") #The dataset is available in Hugginface ine the Salesforce/dialogstudio repository\ndataset # We show the content of the dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> 2. DATA CLEANING <br>\n","metadata":{}},{"cell_type":"markdown","source":"The data comes from twitter conversation, hence contains informations such as usernames preceded by @, urls, or other contextual information. These elements can be identified by looking at some data samples, and removed. </b>","metadata":{}},{"cell_type":"code","source":"def clean_data(data):\n    data = re.sub('http://\\S+', '', data) #Deleting urls\n    data = re.sub('https://\\S+', '', data)\n    data = re.sub(r\"@[^\\s]+\", \"\", data) #Deleting twitter usernames preceded by @\n    data = re.sub('_', ' ', data) #Deleting underscores from the text\n    data = re.sub(r\"\\^[^ ]+\", \"\", data) #Deleting names and initials preceded by ^\n    return data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> 3. FORMATTING THE DATASET </b>","metadata":{}},{"cell_type":"markdown","source":"The dataset contains many information that will not be used during our training, so the content and the format of the dataset will be adjusted to match our needs.","metadata":{}},{"cell_type":"code","source":"# We use the python library Pandas to visualise the training and validation datasets\nimport pandas as pd\n\ntraining_data = pd.DataFrame(dataset[\"train\"]) # The training set\nvalidation_data = pd.DataFrame(dataset[\"validation\"]) # The validation set\ntesting_data = pd.DataFrame(dataset[\"test\"]) # The validation set","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Only the \"log\" column containing the conversation, and the \"original dialog info\" column containing the summaries will be used. So we remove all other columns from the dataset.\n\ntraining_data = training_data.drop(columns=['original dialog id', 'new dialog id','prompt','dialog index']) #Removing unecessary columns from the training data\nvalidation_data = validation_data.drop(columns=['original dialog id', 'new dialog id','prompt','dialog index']) #Removing unecessary columns from the training data\ntesting_data = testing_data.drop(columns=['original dialog id', 'new dialog id','prompt','dialog index']) #Removing unecessary columns from the training data\n\nsummary_column = training_data[\"original dialog info\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nThe log column contains several data entries, we select only the data entries we will need, \"user utterance\" containing the user message, and \"system response\" containing the agent response, and we group them into a single column to obtain the full dialog between user and agent.","metadata":{}},{"cell_type":"code","source":"# We create a helper function to update the content of the \"log\" column.\ndef formatting_log(column,data,index):\n    column[index] = data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> We create a function that takes as a parameter a the \"log\" column from the dataset, and updates the content of every row of the column with the text dialog by using the previous helper function. </b>","metadata":{}},{"cell_type":"code","source":"# Function that will be used to format the dialog column of the training and validation data\nimport re\n\ndef format_column(column):\n    i = 0\n    for row in column:\n        text = \"\"\n        for turn in row: # \n            user = clean_data(turn[\"user utterance\"]) # We select the user utterance\n            agent = clean_data(turn[\"system response\"]) # We select system response\n            text += f\"user:{user}\\nagent:{agent}\\n\" # We concatenate them, and add them to the previous ones if any, which results in the full dialog\n        formatting_log(column, text, i) # We change the content of the column to the text dialog\n        i += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Formatting dialog training text using the previous function\ncolumn_log = training_data[\"log\"]\nformat_column(column_log)\nprint(column_log[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Formatting dialog validation text using the previous function\nvalidation_dialog = validation_data[\"log\"]\nformat_column(validation_dialog)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Formatting dialog testing text using the previous function\ntesting_dialog = testing_data[\"log\"]\nformat_column(testing_dialog)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We can now print the first dialog of the validation dataset to make sure that its format is correct.\nprint(validation_dialog[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We create a function used to format summary columns of the training and validation datasets\nimport json\n\ndef format_column_summary(column):\n    i=0\n    for row in column:\n        data = column[i]\n        text = json.loads(data)\n        text = text[\"summaries\"][\"abstractive_summaries\"][0]\n        text = \" \".join(text)\n        column[i] = text\n        i+=1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We use the previously created function to update the content of the \"original dialog info\" column in the training data\nsummary_training = training_data[\"original dialog info\"]\nformat_column_summary(summary_training)\n\n# We print the first row of the training column as a verification\nprint(summary_training[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We do the same for the \"original dialog info\" column in the validation set\nsummary_validation = validation_data[\"original dialog info\"]\nformat_column_summary(summary_validation)\n\n# We print the first row of the validation column as a verification\nprint(summary_validation[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We do the same for the \"original dialog info\" column in the testing set\nsummary_testing = testing_data[\"original dialog info\"]\nformat_column_summary(summary_testing)\n\n# We print the first row of the validation column as a verification\nprint(summary_testing[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We rename the current columns of the dataset to \"dialog\" and \"summary\" for better understandability\n\ntraining_data.rename(columns={'original dialog info': 'summary', 'log': 'dialog'}, inplace=True)\ntraining_data = training_data[['dialog', 'summary']]\n\nvalidation_data.rename(columns={'original dialog info': 'summary', 'log': 'dialog'}, inplace=True)\nvalidation_data = validation_data[['dialog', 'summary']]\n\ntesting_data.rename(columns={'original dialog info': 'summary', 'log': 'dialog'}, inplace=True)\ntesting_data = testing_data[['dialog', 'summary']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We create Data Dictionary containing all sets to be able and push it to Huggingface Hub to use it for the evaluation part.","metadata":{}},{"cell_type":"code","source":"# We create Data Dictionary containing all sets to be able to push it to Huggingface Hub and use it for the evaluation part.\n\nfrom datasets import DatasetDict, Dataset\n\ndataset_training = Dataset.from_pandas(training_data)\ndataset_validation = Dataset.from_pandas(validation_data)\ndataset_testing = Dataset.from_pandas(testing_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We create a dataset dictionary where we store store the training and validation sets\nfrom datasets import DatasetDict, Dataset\n\nfinal_dataset = DatasetDict({\n    'training': dataset_training,\n    'validation' : dataset_validation,\n    'testing' : dataset_testing\n    })","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We visualise the resulting dataset\nfinal_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We store the current dataset to Hugging Face Hub to use it for our the model evaluation\nfinal_dataset.push_to_hub(\"Dialog-Summarization-Dataset\", token=\"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\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> 4. CONVERTING THE DATASET INTO THE REQUIRED INSTRUCTION FORMAT </b>","metadata":{}},{"cell_type":"markdown","source":"To be able to fine-tune the Llama2-7b-chat model, we need a to convert the dataset into the required instruction format for the model. Each row in the instruction dataset should be in the format:\n<s> [INST] <<SYS>> {{system_prompt}} <</SYS>> {{input}} [/INST] {{summary}} </s>\nWhere: {{ system_prompt }} represents the default prompt used in the dataset, {{ input }} represents the dialog to be summarised, and summary represents the corresponding summary of the dialog.\nSo we need to reformat the current dataset to match this specific format.","metadata":{}},{"cell_type":"code","source":"# We create a default_prompt that will be used as a system prompt in the instruction dataset\ndefault_prompt = \"The following text is a conversation between a user and an AI agent. Write a summary of the conversation.\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We convert the training set into the instruction format for the Llama2 chat model\nimport pandas as pd\n\ndf_training = pd.DataFrame(dataset_training)\n\ndf_training['text'] = df_training.apply(\n    lambda row: f\"\"\"<s> [INST] <<SYS>> {default_prompt} <</SYS>> {row['dialog']} [/INST] {row['summary']} </s>\"\"\",axis=1\n)\n\ndataset_training = df_training[['text']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We convert the training set into the instruction format for the Llama2 chat model\ndf_validation = pd.DataFrame(dataset_validation)\n\ndf_validation['text'] = df_validation.apply(\n    lambda row: f\"\"\"<s> [ INST] <<SYS>> {default_prompt} <</SYS>> {row['dialog']} [/INST] {row['summary']} </s>\"\"\",axis=1\n)\n\ndataset_validation = df_validation[['text']]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We convert the datasets from pandas into the format required to store it  used in Hugging Face Hub, the testing set will not be used during the training\ndataset_training_formatted = Dataset.from_pandas(dataset_training)\ndataset_validation_formatted = Dataset.from_pandas(dataset_validation)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We create a dataset dictionary where we store store the training and validation sets\nfrom datasets import DatasetDict, Dataset\n\nfinal_dataset_formatted = DatasetDict({\n    'training': dataset_training_formatted,\n    'validation' : dataset_validation_formatted\n    })\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We visualise content of the dataset dictionary \nfinal_dataset_formatted","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our final dataset is now ready, we can now push it to Hugging Face Hub","metadata":{}},{"cell_type":"code","source":"# We store the final formatted dataset in Hugging Face Hub to use it for finetuning.\nfinal_dataset_formatted.push_to_hub(\"Dialog-Summarization-Dataset-Formatted\", token=\"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\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Reference: TweetSum Dataset https://huggingface.co/datasets/Salesforce/dialogstudio/tree/main/dialogue_summarization/TweetSumm","metadata":{}}]}
